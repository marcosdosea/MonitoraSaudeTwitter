{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WagnerPrata/Minera-o-de-dados-no-Twitter-/blob/main/V5_Analise_de_Sentimentos_Tweets_COVID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O2Z7YtCVubm"
      },
      "source": [
        "# **Instalações das Bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is4VeIYnEuAv"
      },
      "outputs": [],
      "source": [
        "pip install tweepy textblob matplotlib wordcloud googletrans unidecode -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6W9uUzYKNx9"
      },
      "outputs": [],
      "source": [
        "pip install TwitterSearch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q0uXJzaCVqf"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade textblob -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2HhMyy7uHyC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ4Fz8tbFY0t"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import json\n",
        "from textblob import TextBlob\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "from googletrans import Translator\n",
        "from unidecode import unidecode\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqboSL-LOV1Q",
        "outputId": "916a1e0f-530f-466b-d0a8-ae1802eb8864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm47f-hdKj9O"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srNG16bkFdMk"
      },
      "outputs": [],
      "source": [
        "consumer_key = \"cccc\" \n",
        "consumer_secret = \"ccc\"\n",
        "access_token = \"cccc\"\n",
        "access_token_secret = \"cccc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWPvNBIqC0pY"
      },
      "outputs": [],
      "source": [
        "#Criando nossa autenticação\n",
        "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
        "#\"Direcionando\" o token de acesso\n",
        "auth.set_access_token(access_token, access_token_secret) \n",
        "#criando o objeto da API usando nossas credenciais\n",
        "api = tweepy.API(auth, wait_on_rate_limit = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPebJ4cdpxh_"
      },
      "outputs": [],
      "source": [
        "language = 'pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgdQGJMdV87k"
      },
      "source": [
        "# **Extração de dados Twiiter** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd2LZuFLCr1b"
      },
      "outputs": [],
      "source": [
        "from TwitterSearch import *\n",
        "\n",
        "#CHAVES DE ACESSO TWITTER\n",
        "try:\n",
        "    ts = TwitterSearch(\n",
        "        consumer_key = consumer_key ,\n",
        "        consumer_secret = consumer_secret,\n",
        "        access_token = access_token,\n",
        "        access_token_secret = access_token_secret\n",
        "    )\n",
        "    \n",
        "    #ATRIBUTOS DA PESQUISA\n",
        "   \n",
        "    tso = TwitterSearchOrder()\n",
        "    #tso.set_geocode(-11.0058313,-37.1731961,25, imperial_metric=False)\n",
        "    tso.set_keywords(['since:2022-10-15','2022-10-08'])         \n",
        "    tso.set_keywords(['UniversidadeEuDefendo','CienciaEuDefendo','EducacaoEuDefendo'], or_operator = True) #Aqui serão as palavras chaves tso.set_keywords(['vacina','vírus', 'coronavírus'], or_operator = True) #Aqui serão as palavras chaves \n",
        "    tso.set_language('pt') \n",
        "    cont = 1 #condicao de parada, quantidade de resultados\n",
        "\n",
        "    for tweet in ts.search_tweets_iterable(tso):\n",
        "        print('created_at: ',tweet['created_at'], 'User_id: ',tweet['id_str'], 'Tweet: ',tweet['text'] ) \n",
        "        created_at = tweet['created_at']\n",
        "        user_id = tweet['id_str']\n",
        "        texto = tweet['text']\n",
        "        \n",
        "        cont += 1\n",
        "        if cont>10000: #condicao de parada, quantidade de resultados\n",
        "            break\n",
        "\n",
        "        with open(\"dado.json\", \"a+\") as output:\n",
        "\n",
        "          data = {\"created_at\": created_at,\n",
        "                  \"User_id\": user_id,\n",
        "                  \"Tweets\": texto}\n",
        "          output.write(\"{}\\n\".format(json.dumps(data)))\n",
        "\n",
        "except TwitterSearchException as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXnw5EdHUn36"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json('dado.json', lines = True)\n",
        "#df.head(10) #mostrando 10 resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXFZRkh9R9tX"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYUywhEbiWSs"
      },
      "outputs": [],
      "source": [
        "#Duplica a base da dados para CSV\n",
        "df.to_csv (\"dado.csv\")\n",
        "#df.head(10) #mostrando 10 resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_-toY58b_B8"
      },
      "outputs": [],
      "source": [
        "dados = pd.read_csv('/content/dado.csv') #lendo a base de dados duplicada\n",
        "#dados.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hps1iZfZVciT"
      },
      "source": [
        "# **Limpeza de Dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDqRxu4-YYpk"
      },
      "outputs": [],
      "source": [
        "dados = pd.DataFrame(dados, columns = ['Tweets'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ugwpcmTD09V"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74JtdVCkG0cV"
      },
      "outputs": [],
      "source": [
        "def limpando_chars(text):\n",
        "    text = re.sub('@[A-Za-z0–9]+', '', text) #removendo @\n",
        "    text = re.sub('#', '', text) #removendo #\n",
        "    text = re.sub('RT[\\s]+', '', text) # Removendo RT\n",
        "    text = re.sub('https?:\\/\\/\\S+', '', text) # Removendo hyperlink\n",
        "    text = re.sub('&amp','', text)# removendo marcação HTML de início\n",
        "    text = re.sub(r\"[^a-zA-Zà-úÀ-Ú0-9]\", \" \", text.lower()) # Converte para minúsculo\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbQwHx5XFu2t"
      },
      "outputs": [],
      "source": [
        "#limpando os tweets com a função criada\n",
        "dados['Tweets'] = dados['Tweets'].apply(limpando_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii0aIMAFZjtE"
      },
      "outputs": [],
      "source": [
        "dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzaladVZASPV"
      },
      "outputs": [],
      "source": [
        "#Copiando o dataframe(com dados limpos) para usar no nuvem de palvras\n",
        "Word_Cloud = pd.DataFrame(dados, columns = ['Tweets'])\n",
        "dados = Word_Cloud\n",
        "Word_Cloud.to_csv (\"Nuvem.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGIudoAoVkRs"
      },
      "source": [
        "# **Preparando para análise - Tradução** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n4bblYdSDEN"
      },
      "outputs": [],
      "source": [
        "!pip install translate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0Xeu_PWSG7N"
      },
      "outputs": [],
      "source": [
        "from translate import Translator\n",
        "from textblob import TextBlob\n",
        "import translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMPlcYXtYgBs"
      },
      "outputs": [],
      "source": [
        "for index, row in dados.iterrows():\n",
        "    pt_blob = dados.iloc[index]['Tweets']\n",
        "    translator = Translator(from_lang=\"pt\", to_lang=\"en\")\n",
        "    #translator = Translator(to_lang=\"en\") #Traduzir para Inglês    pt_blob = translator.translate(pt_blob)\n",
        "    en_blob = translator.translate(pt_blob)\n",
        "    dados.at[index, str('Tweets')] = str(en_blob)\n",
        "dados.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne7MfogHinH1"
      },
      "outputs": [],
      "source": [
        "#trandformo o dataframe em um arquivo CSV - para aplicar a analise \n",
        "dados.to_csv(\"traducao.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D_mmxqzqSsN"
      },
      "outputs": [],
      "source": [
        "#Cria um dataframe especifico para analise \n",
        "analise = pd.read_csv(\"traducao.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8W5afUZWVcz"
      },
      "source": [
        "# **Análise de Twiiter Negativos de Positivos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS0Y5yCF-zwR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score #acuracia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBALJI2j-rlB"
      },
      "outputs": [],
      "source": [
        "# Função que analisa e obtém a subjetividade\n",
        "def capture_subjetividade_pt(text):\n",
        "    return TextBlob(text).sentiment.subjectivity\n",
        "# Função que analisa e obtém a polaridade\n",
        "def capture_polaridade_pt(text):\n",
        "    return  TextBlob(text).sentiment.polarity\n",
        "\n",
        "#def translate_text(text): #Aqui encontramos limitações na quantidade de traduções \n",
        "      #translator = Translator(from_lang=\"pt\", to_lang=\"en\") #Traduzir de Portugues para Ingles\n",
        "      #return translator.translate(text)\n",
        "\n",
        "#Traduzindo\n",
        "#df_tweets['Tweets'] = df_tweets['Tweets'].apply(translate_text)\n",
        "\n",
        "# Criar duas colunas de subjetividade e polaridade\n",
        "\n",
        "analise['Subjetividade'] = analise['Tweets'].apply(capture_polaridade_pt)\n",
        "analise['Polaridade'] = analise['Tweets'].apply(capture_subjetividade_pt)\n",
        "\n",
        "# Mostra um novo dataframe com as colunas subjetividade e polaridade\n",
        "#analise.tail(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G793TzSdqt_I"
      },
      "outputs": [],
      "source": [
        "# Mostra um novo dataframe com as colunas subjetividade e polaridade\n",
        "analise.tail(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hhxK04NFzfm"
      },
      "outputs": [],
      "source": [
        "#-1 negativo\n",
        "#0 neutro\n",
        "#1 positivo\n",
        "def gerar_analise(score):\n",
        "    if score < 0:\n",
        "        return 'Negativo'\n",
        "    elif score == 0:\n",
        "        return 'Negativo'\n",
        "    else:\n",
        "        return 'Positivo'\n",
        "analise['Analise'] = analise['Polaridade'].apply(gerar_analise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6E1WqYIriN-"
      },
      "outputs": [],
      "source": [
        "analise.to_csv(\"Analise.csv\") #Criando um arquivo CSV das Analise "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbqgt0-MiIs0"
      },
      "outputs": [],
      "source": [
        "analise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYVhq7i5iEx9"
      },
      "outputs": [],
      "source": [
        "from seaborn.categorical import boxplot\n",
        "import matplotlib.pyplot as plt\n",
        "analise.boxplot(column= ['Polaridade'])\n",
        "plt.savefig('Boxplot_Polaridade.png', format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COts3NERj2Gv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "analise.boxplot(column= ['Subjetividade'])\n",
        "plt.savefig('Boxplot_Subjetividade.png', format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAK5vNZFF1H-"
      },
      "outputs": [],
      "source": [
        "# Printando apenas os tweets positivos \n",
        "print('Tweets Positivos:\\n')\n",
        "j=1\n",
        "sortedDF = analise.sort_values(by=['Polaridade']) #ordenando os tweets\n",
        "for i in range(0, sortedDF.shape[0] ):\n",
        "    if( sortedDF['Analise'][i] == 'Positivo'):\n",
        "        print(str(j) + ') '+ sortedDF['Tweets'][i])\n",
        "        print()\n",
        "        j= j+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QPE-VSnF4BM"
      },
      "outputs": [],
      "source": [
        "# Printando apenas os tweets negativos \n",
        "print('Tweets Negativos:\\n')\n",
        "j=1\n",
        "sortedDF = analise.sort_values(by=['Polaridade'],ascending=False) #ordenando os tweets\n",
        "for i in range(0, sortedDF.shape[0] ):\n",
        "    if( sortedDF['Analise'][i] == 'Negativo'):\n",
        "        print(str(j) + ') '+sortedDF['Tweets'][i])\n",
        "        print()\n",
        "        j=j+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvO0kAiyF6Pl"
      },
      "outputs": [],
      "source": [
        "# Plotting \n",
        "plt.figure(figsize=(8,6)) \n",
        "for i in range(0, analise.shape[0]):\n",
        "    plt.scatter(analise[\"Polaridade\"][i], analise[\"Subjetividade\"][i], color='darkblue') \n",
        "plt.title('Análise de sentimento') \n",
        "plt.xlabel('Polaridade') \n",
        "plt.ylabel('Subjetividade')\n",
        "plt.savefig('Gráfico de dispersão.png', format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt-LZzhaF777",
        "outputId": "79fdc804-24d5-4436-971b-ad0519f7dd4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "#Percentual de Tweets positivos\n",
        "ptweets = analise[analise.Analise == 'Positivo']\n",
        "ptweets = ptweets['Tweets']\n",
        "ptweets\n",
        "\n",
        "round( (ptweets.shape[0] / analise.shape[0]) * 100 , 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk-aWYcrF9vc",
        "outputId": "ea5e3151-f42c-4948-e23a-3fde18dde596"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.3"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "#Percentual de Tweets Negativo\n",
        "ntweets = analise[analise.Analise == 'Negativo']\n",
        "ntweets = ntweets['Tweets']\n",
        "ntweets\n",
        "\n",
        "round( (ntweets.shape[0] / analise.shape[0]) * 100, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBSvPjujF_d0"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(analise['Analise'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThAFics2GBcS"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "ax = sns.barplot(x=df['Analise'], y=df.index, data=analise)\n",
        "ax.set_xlabel('Quantidade de tweets')\n",
        "ax.set_ylabel('Sentimento');\n",
        "\n",
        "plt.savefig('Barra01.png', format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCNtxMFZGDDM"
      },
      "outputs": [],
      "source": [
        "ax = sns.barplot(x=df.index, y=df['Analise'],data=analise,label='Quantidade de Tweets')\n",
        "ax.set_ylabel('Quantidade de tweets')\n",
        "ax.set_xlabel('Sentimento');\n",
        "plt.savefig('Gráfico Barra 02.png', format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hzpF9QEAQaq"
      },
      "source": [
        "# **Nuvem de palavras**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjPxvcvZxmXl"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vWKdiBvz4c0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/Nuvem.csv\") #lendo a base de dados duplicada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZygVxVH4ph3",
        "outputId": "cbf0c7cd-49b9-485f-d1c8-b11752618cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de Palavras: 9417\n"
          ]
        }
      ],
      "source": [
        "# concatenar as frases\n",
        "all_frases = \" \".join(s for s in df['Tweets'])\n",
        "print(\"Quantidade de Palavras: {}\".format(len(all_frases)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOpMN9Xi0ifF"
      },
      "outputs": [],
      "source": [
        "#Palavras indesejaveis e sem sentido\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update(['eu','ver','principal','essa','isso','vez','nas','mas','qual','principal','ele','ter','doença','pois','este','vez','ver principal','artigo principal','já','aos','pode','outro','artigo','desse','alguns','meio','entre','das','podem','esse','seu','também','são','quando','de', 'que','em','os','as','da','como','dos','ou','se','um','uma','para','na','ao','mais','por','não','ainda','muito','sua'] + list(STOPWORDS) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlXwizCY5Qrb"
      },
      "outputs": [],
      "source": [
        "# configurando a wordcloud\n",
        "wordcloud = WordCloud(stopwords=stopwords,\n",
        "                      background_color=\"white\",\n",
        "                      width=1600, height=800).generate(all_frases)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "ax.imshow(wordcloud, interpolation='bilinear')\n",
        "ax.set_axis_off()\n",
        "\n",
        "# mostrar a imagem\n",
        "plt.imshow(wordcloud);\n",
        "\n",
        "# salvar a imagem no diretório\n",
        "wordcloud.to_file(\"wordcloud.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUGnUh7GiaNv"
      },
      "source": [
        "#**Tokeniza**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut4vTbziid1X"
      },
      "outputs": [],
      "source": [
        "#import Countvectorizar para cantar numero de vezes que uma palavra ocorre\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_VWk2_HighO"
      },
      "outputs": [],
      "source": [
        "#Crie uma matriz para mostrar o número de vezes que palavras específicas aparecem no texto da coluna\n",
        "cv = CountVectorizer()\n",
        "count_matriz = cv.fit_transform(df.Tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr8vtX92ip5m"
      },
      "outputs": [],
      "source": [
        "word_count = pd.DataFrame(cv.get_feature_names(), columns=['word'])\n",
        "word_count[\"count\"] = count_matriz.sum(axis = 0).tolist()[0]\n",
        "word_count = word_count.sort_values(\"count\", ascending = False).reset_index(drop =True)\n",
        "word_count[:100]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quif-Hs1XFES"
      },
      "source": [
        "# **Futuras melhorias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcwIfBFXV9Ub"
      },
      "outputs": [],
      "source": [
        "def RemoveStopWords(instancia):\n",
        "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
        "    palavras = [i for i in instancia.split() if not i in stopwords]\n",
        "    return (\" \".join(palavras))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwDxS7UeWM2B"
      },
      "outputs": [],
      "source": [
        "def Stemming(instancia):\n",
        "    stemmer = nltk.stem.RSLPStemmer()\n",
        "    palavras = []\n",
        "    for w in instancia.split():\n",
        "        palavras.append(stemmer.stem(w))\n",
        "    return (\" \".join(palavras))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSBMm1VEWRPJ"
      },
      "outputs": [],
      "source": [
        "def Limpeza_dados(instancia):\n",
        "    # remove links, pontos, virgulas,ponto e virgulas dos tweets\n",
        "    instancia = re.sub(r\"http\\S+\", \"\", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')\n",
        "    return (instancia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHAHuONFZMQj"
      },
      "outputs": [],
      "source": [
        "def Preprocessing(text):\n",
        "    stemmer = nltk.stem.RSLPStemmer()\n",
        "    instancia = re.sub(r\"http\\S+\", \"\", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')\n",
        "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
        "    palavras = [stemmer.stem(i) for i in instancia.split() if not i in stopwords]\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNgaTixajuhK"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def tweet_to_words(tweet):\n",
        "    \n",
        "    text = BeautifulSoup(tweet, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Zà-úÀ-Ú0-9]\", \" \", text.lower()) # Converte para minúsculo\n",
        "    words = tweet_tokenizer.tokenize(text)\n",
        "    \n",
        "    return words"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
